{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cell-width control\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#packages\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "#utils\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import struct\n",
    "import time\n",
    "from generators import *\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Concatenate, Dot, Embedding, LSTM, Conv1D, MaxPooling1D, Input, Lambda\n",
    "    #callbacks\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, Callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 400000\n",
    "maxlen_text = 400\n",
    "maxlen_summ = 80\n",
    "embedding_size = 100 #128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###tw_on_pseudorandom###\n",
      "on 100% clean\tloss:\t0.15479131675426508\tacc:\t0.9424616991643454\n",
      "on 100% pseudo\tloss:\t0.4951415386798073\tacc:\t0.804491643454039\n",
      "on 100% gen\tloss:\t1.6653959887653755\tacc:\t0.34479748603351956\n",
      "on 100% uni\tloss:\t0.013805881002528754\tacc:\t1.0\n",
      "###tw_on_generator###\n",
      "on 100% clean\tloss:\t0.25931909057755326\tacc:\t0.8840529247910863\n",
      "on 100% pseudo\tloss:\t2.497066032919711\tacc:\t0.4257486072423398\n",
      "on 100% gen\tloss:\t0.2094455855643616\tacc:\t0.9208275139664804\n",
      "on 100% uni\tloss:\t0.790658339650518\tacc:\t0.5366469359331476\n",
      "###tw_on_uniform###\n",
      "on 100% clean\tloss:\t1.192093321833454e-07\tacc:\t1.0\n",
      "on 100% pseudo\tloss:\t15.942384719848633\tacc:\t0.0\n",
      "on 100% gen\tloss:\t15.942384719848633\tacc:\t0.0\n",
      "on 100% uni\tloss:\t1.0000001537946446e-07\tacc:\t1.0\n",
      "###ow_on_pseudorandom###\n",
      "on 100% clean\tloss:\t0.33629784407223834\tacc:\t0.9399373259052924\n",
      "on 100% pseudo\tloss:\t0.5030146239423984\tacc:\t0.671483286908078\n",
      "on 100% gen\tloss:\t0.8535185221520216\tacc:\t0.4075593575418994\n",
      "on 100% uni\tloss:\t0.0019344505393478117\tacc:\t1.0\n",
      "###ow_on_generator###\n",
      "on 100% clean\tloss:\t0.2539331919843108\tacc:\t0.8856197771587744\n",
      "on 100% pseudo\tloss:\t2.99679205809463\tacc:\t0.3963266016713092\n",
      "on 100% gen\tloss:\t0.23481662302959563\tacc:\t0.9094797486033519\n",
      "on 100% uni\tloss:\t2.183413243891469\tacc:\t0.12047353760445682\n",
      "###ow_on_uniform###\n",
      "on 100% clean\tloss:\t1.192093321833454e-07\tacc:\t1.0\n",
      "on 100% pseudo\tloss:\t15.942384719848633\tacc:\t0.0\n",
      "on 100% gen\tloss:\t15.942384719848633\tacc:\t0.0\n",
      "on 100% uni\tloss:\t1.0000001537946446e-07\tacc:\t1.0\n"
     ]
    }
   ],
   "source": [
    "use_multiprocessing = True\n",
    "workers = 10\n",
    "shuffle = False\n",
    "\n",
    "ow_on_pseudorandom = ['ow_on_pseudorandom','/home/donald/documents/MT/implementation-and-experiments/exciting-crazy/experiments/ow-on-pseudorandom/1/best.h5']\n",
    "ow_on_generator = ['ow_on_generator','/home/donald/documents/MT/implementation-and-experiments/exciting-crazy/experiments/ow-on-generator/1/best.h5']\n",
    "ow_on_uniform = ['ow_on_uniform','/home/donald/documents/MT/implementation-and-experiments/exciting-crazy/experiments/ow-on-uniform/1/best.h5']\n",
    "tw_on_pseudorandom = ['tw_on_pseudorandom','/home/donald/documents/MT/implementation-and-experiments/exciting-crazy/experiments/tw-on-pseudorandom/1/best.h5']\n",
    "tw_on_generator = ['tw_on_generator','/home/donald/documents/MT/implementation-and-experiments/exciting-crazy/experiments/tw-on-generator/1/best.h5']\n",
    "tw_on_uniform = ['tw_on_uniform','/home/donald/documents/MT/implementation-and-experiments/exciting-crazy/experiments/tw-on-uniform/1/best.h5']\n",
    "\n",
    "model_paths = [tw_on_pseudorandom, tw_on_generator, tw_on_uniform, ow_on_pseudorandom, ow_on_generator, ow_on_uniform]\n",
    "\n",
    "#get preprocessing data\n",
    "processing_dir = '/mnt/disks/500gb/stats-and-meta-data/400000/'\n",
    "with open(processing_dir+'tokenizer.pickle', 'rb') as handle: tokenizer = pickle.load(handle)\n",
    "embedding_matrix = numpy.load(processing_dir+'embedding_matrix.npy')\n",
    "#stats\n",
    "maxi = numpy.load(processing_dir+'training-stats-all/maxi.npy')\n",
    "mini = numpy.load(processing_dir+'training-stats-all/mini.npy')\n",
    "sample_info = (numpy.random.uniform, mini,maxi)\n",
    "\n",
    "\n",
    "for model_path in model_paths:\n",
    "    model_name = model_path[0]\n",
    "    model = load_model(model_path[1])\n",
    "    \n",
    "    print('###'+model_name+'###')\n",
    "    \n",
    "    if model_name[0:2] == 'tw':\n",
    "        #eval on clean test\n",
    "        data_dir = '/mnt/disks/500gb/experimental-data-mini/experimental-data-mini/evaluation-data/test-onlyclean/only-clean/'\n",
    "        with open(data_dir+'partition.pickle', 'rb') as handle: partition = pickle.load(handle)\n",
    "        with open(data_dir+'labels.pickle', 'rb') as handle: labels = pickle.load(handle)\n",
    "        #batch generator parameters\n",
    "        params = {'dim': [(maxlen_text,embedding_size),(maxlen_summ,embedding_size)],\n",
    "                  'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                 'tokenizer':tokenizer,\n",
    "                 'embedding_matrix':embedding_matrix,\n",
    "                 'maxlen_text':maxlen_text,\n",
    "                 'maxlen_summ':maxlen_summ,\n",
    "                 'data_dir':data_dir,\n",
    "                 'sample_info':sample_info}\n",
    "        #generators\n",
    "        test_generator = ContAllGenerator(partition['test'], labels, **params)\n",
    "        # Train model on dataset\n",
    "        out = model.evaluate_generator(generator=test_generator,\n",
    "                            use_multiprocessing=use_multiprocessing,\n",
    "                            workers=workers)\n",
    "        print('on 100% clean\\t'+'loss:\\t'+str(out[0])+'\\tacc:\\t'+str(out[1]))\n",
    "\n",
    "        #eval on pseudorandom-noise\n",
    "        data_dir = '/mnt/disks/500gb/experimental-data-mini/experimental-data-mini/evaluation-data/test-onlynoise/pseudorandom-dist/only-noise/'\n",
    "        with open(data_dir+'partition.pickle', 'rb') as handle: partition = pickle.load(handle)\n",
    "        with open(data_dir+'labels.pickle', 'rb') as handle: labels = pickle.load(handle)\n",
    "        #batch generator parameters\n",
    "        params = {'dim': [(maxlen_text,embedding_size),(maxlen_summ,embedding_size)],\n",
    "                  'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                 'tokenizer':tokenizer,\n",
    "                 'embedding_matrix':embedding_matrix,\n",
    "                 'maxlen_text':maxlen_text,\n",
    "                 'maxlen_summ':maxlen_summ,\n",
    "                 'data_dir':data_dir,\n",
    "                 'sample_info':sample_info}\n",
    "        #generators\n",
    "        test_generator = ContAllGenerator(partition['test'], labels, **params)\n",
    "        # Train model on dataset\n",
    "        out = model.evaluate_generator(generator=test_generator,\n",
    "                            use_multiprocessing=use_multiprocessing,\n",
    "                            workers=workers)\n",
    "        print('on 100% pseudo\\t'+'loss:\\t'+str(out[0])+'\\tacc:\\t'+str(out[1]))\n",
    "\n",
    "        #eval on generator noise\n",
    "        data_dir = '/mnt/disks/500gb/experimental-data-mini/experimental-data-mini/evaluation-data/test-onlynoise/generator-dist/only-noise/'\n",
    "        with open(data_dir+'partition.pickle', 'rb') as handle: partition = pickle.load(handle)\n",
    "        with open(data_dir+'labels.pickle', 'rb') as handle: labels = pickle.load(handle)\n",
    "        #batch generator parameters\n",
    "        params = {'dim': [(maxlen_text,embedding_size),(maxlen_summ,embedding_size)],\n",
    "                  'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                 'tokenizer':tokenizer,\n",
    "                 'embedding_matrix':embedding_matrix,\n",
    "                 'maxlen_text':maxlen_text,\n",
    "                 'maxlen_summ':maxlen_summ,\n",
    "                 'data_dir':data_dir,\n",
    "                 'sample_info':sample_info}\n",
    "        #generators\n",
    "        test_generator = ContAllGenerator(partition['test'], labels, **params)\n",
    "        # Train model on dataset\n",
    "        out = model.evaluate_generator(generator=test_generator,\n",
    "                            use_multiprocessing=use_multiprocessing,\n",
    "                            workers=workers)\n",
    "        print('on 100% gen\\t'+'loss:\\t'+str(out[0])+'\\tacc:\\t'+str(out[1]))\n",
    "\n",
    "        #eval on clean test\n",
    "        data_dir = '/mnt/disks/500gb/experimental-data-mini/experimental-data-mini/evaluation-data/test-onlynoise/pseudorandom-dist/only-noise/'\n",
    "        with open(data_dir+'partition.pickle', 'rb') as handle: partition = pickle.load(handle)\n",
    "        with open(data_dir+'labels.pickle', 'rb') as handle: labels = pickle.load(handle)\n",
    "        #batch generator parameters\n",
    "        params = {'dim': [(maxlen_text,embedding_size),(maxlen_summ,embedding_size)],\n",
    "                  'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                 'tokenizer':tokenizer,\n",
    "                 'embedding_matrix':embedding_matrix,\n",
    "                 'maxlen_text':maxlen_text,\n",
    "                 'maxlen_summ':maxlen_summ,\n",
    "                 'data_dir':data_dir,\n",
    "                 'sample_info':sample_info}\n",
    "        #generators\n",
    "        test_generator = TwoQuartGenerator(partition['test'], labels, **params)\n",
    "        # Train model on dataset\n",
    "        out = model.evaluate_generator(generator=test_generator,\n",
    "                            use_multiprocessing=use_multiprocessing,\n",
    "                            workers=workers)\n",
    "        print('on 100% uni\\t'+'loss:\\t'+str(out[0])+'\\tacc:\\t'+str(out[1]))\n",
    "    elif model_name[0:2] == 'ow':\n",
    "        #eval on clean test\n",
    "        data_dir = '/mnt/disks/500gb/experimental-data-mini/experimental-data-mini/evaluation-data/test-onlyclean/only-clean/'\n",
    "        with open(data_dir+'partition.pickle', 'rb') as handle: partition = pickle.load(handle)\n",
    "        with open(data_dir+'labels.pickle', 'rb') as handle: labels = pickle.load(handle)\n",
    "        #batch generator parameters\n",
    "        params = {'dim': [(maxlen_text,embedding_size),(maxlen_summ,embedding_size)],\n",
    "                  'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                 'tokenizer':tokenizer,\n",
    "                 'embedding_matrix':embedding_matrix,\n",
    "                 'maxlen_text':maxlen_text,\n",
    "                 'maxlen_summ':maxlen_summ,\n",
    "                 'data_dir':data_dir,\n",
    "                 'sample_info':sample_info}\n",
    "        #generators\n",
    "        test_generator = ContAllGenerator_ow(partition['test'], labels, **params)\n",
    "        # Train model on dataset\n",
    "        out = model.evaluate_generator(generator=test_generator,\n",
    "                            use_multiprocessing=use_multiprocessing,\n",
    "                            workers=workers)\n",
    "        print('on 100% clean\\t'+'loss:\\t'+str(out[0])+'\\tacc:\\t'+str(out[1]))\n",
    "\n",
    "        #eval on pseudorandom-noise\n",
    "        data_dir = '/mnt/disks/500gb/experimental-data-mini/experimental-data-mini/evaluation-data/test-onlynoise/pseudorandom-dist/only-noise/'\n",
    "        with open(data_dir+'partition.pickle', 'rb') as handle: partition = pickle.load(handle)\n",
    "        with open(data_dir+'labels.pickle', 'rb') as handle: labels = pickle.load(handle)\n",
    "        #batch generator parameters\n",
    "        params = {'dim': [(maxlen_text,embedding_size),(maxlen_summ,embedding_size)],\n",
    "                  'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                 'tokenizer':tokenizer,\n",
    "                 'embedding_matrix':embedding_matrix,\n",
    "                 'maxlen_text':maxlen_text,\n",
    "                 'maxlen_summ':maxlen_summ,\n",
    "                 'data_dir':data_dir,\n",
    "                 'sample_info':sample_info}\n",
    "        #generators\n",
    "        test_generator = ContAllGenerator_ow(partition['test'], labels, **params)\n",
    "        # Train model on dataset\n",
    "        out = model.evaluate_generator(generator=test_generator,\n",
    "                            use_multiprocessing=use_multiprocessing,\n",
    "                            workers=workers)\n",
    "        print('on 100% pseudo\\t'+'loss:\\t'+str(out[0])+'\\tacc:\\t'+str(out[1]))\n",
    "\n",
    "        #eval on generator noise\n",
    "        data_dir = '/mnt/disks/500gb/experimental-data-mini/experimental-data-mini/evaluation-data/test-onlynoise/generator-dist/only-noise/'\n",
    "        with open(data_dir+'partition.pickle', 'rb') as handle: partition = pickle.load(handle)\n",
    "        with open(data_dir+'labels.pickle', 'rb') as handle: labels = pickle.load(handle)\n",
    "        #batch generator parameters\n",
    "        params = {'dim': [(maxlen_text,embedding_size),(maxlen_summ,embedding_size)],\n",
    "                  'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                 'tokenizer':tokenizer,\n",
    "                 'embedding_matrix':embedding_matrix,\n",
    "                 'maxlen_text':maxlen_text,\n",
    "                 'maxlen_summ':maxlen_summ,\n",
    "                 'data_dir':data_dir,\n",
    "                 'sample_info':sample_info}\n",
    "        #generators\n",
    "        test_generator = ContAllGenerator_ow(partition['test'], labels, **params)\n",
    "        # Train model on dataset\n",
    "        out = model.evaluate_generator(generator=test_generator,\n",
    "                            use_multiprocessing=use_multiprocessing,\n",
    "                            workers=workers)\n",
    "        print('on 100% gen\\t'+'loss:\\t'+str(out[0])+'\\tacc:\\t'+str(out[1]))\n",
    "\n",
    "        #eval on clean test\n",
    "        data_dir = '/mnt/disks/500gb/experimental-data-mini/experimental-data-mini/evaluation-data/test-onlynoise/pseudorandom-dist/only-noise/'\n",
    "        with open(data_dir+'partition.pickle', 'rb') as handle: partition = pickle.load(handle)\n",
    "        with open(data_dir+'labels.pickle', 'rb') as handle: labels = pickle.load(handle)\n",
    "        #batch generator parameters\n",
    "        params = {'dim': [(maxlen_text,embedding_size),(maxlen_summ,embedding_size)],\n",
    "                  'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                 'tokenizer':tokenizer,\n",
    "                 'embedding_matrix':embedding_matrix,\n",
    "                 'maxlen_text':maxlen_text,\n",
    "                 'maxlen_summ':maxlen_summ,\n",
    "                 'data_dir':data_dir,\n",
    "                 'sample_info':sample_info}\n",
    "        #generators\n",
    "        test_generator = TwoQuartGenerator_ow(partition['test'], labels, **params)\n",
    "        # Train model on dataset\n",
    "        out = model.evaluate_generator(generator=test_generator,\n",
    "                            use_multiprocessing=use_multiprocessing,\n",
    "                            workers=workers)\n",
    "        print('on 100% uni\\t'+'loss:\\t'+str(out[0])+'\\tacc:\\t'+str(out[1]))\n",
    "    else:\n",
    "        print('wrong model')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
