{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cell-width control\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#packages\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "#utils\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import struct\n",
    "import time\n",
    "from generators import *\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Concatenate, Dot, Embedding, LSTM, Conv1D, MaxPooling1D, Input, Lambda\n",
    "    #callbacks\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, Callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sd = 1\n",
    "from numpy.random import seed\n",
    "seed(sd)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 400000\n",
    "maxlen_text = 400\n",
    "maxlen_summ = 80\n",
    "embedding_size = 100 #128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/mnt/disks/500gb/experimental-data-mini/experimental-data-mini/generator-dist-1to1/1to1/'\n",
    "#data_dir = '/media/oala/4TB/experimental-data/experiment-1_nonconform-models/generator-dist/1to1/'\n",
    "processing_dir = '/mnt/disks/500gb/stats-and-meta-data/400000/'\n",
    "#processing_dir = '/media/oala/4TB/experimental-data/stats-and-meta-data/400000/'\n",
    "\n",
    "with open(data_dir+'partition.pickle', 'rb') as handle: partition = pickle.load(handle)\n",
    "with open(data_dir+'labels.pickle', 'rb') as handle: labels = pickle.load(handle)\n",
    "\n",
    "with open(processing_dir+'tokenizer.pickle', 'rb') as handle: tokenizer = pickle.load(handle)\n",
    "embedding_matrix = numpy.load(processing_dir+'embedding_matrix.npy')\n",
    "\n",
    "#the p_n constant\n",
    "c = 80000\n",
    "\n",
    "#stats\n",
    "maxi = numpy.load(processing_dir+'training-stats-all/maxi.npy')\n",
    "mini = numpy.load(processing_dir+'training-stats-all/mini.npy')\n",
    "sample_info = (numpy.random.uniform, mini,maxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2way input\n",
    "text_input = Input(shape=(maxlen_text,embedding_size), dtype='float32')\n",
    "summ_input = Input(shape=(maxlen_summ,embedding_size), dtype='float32')\n",
    "\n",
    "#1way dropout\n",
    "#text_route = Dropout(0.25)(text_input)\n",
    "summ_route = Dropout(0.25)(summ_input)\n",
    "\n",
    "#1way conv\n",
    "#text_route = Conv1D(filters,\n",
    "                 #kernel_size,\n",
    "                 #padding='valid',\n",
    "                 #activation='relu',\n",
    "                 #strides=1)(text_route)\n",
    "summ_route = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(summ_route)\n",
    "\n",
    "#1way max pool\n",
    "#text_route = MaxPooling1D(pool_size=pool_size)(text_route)\n",
    "summ_route = MaxPooling1D(pool_size=pool_size)(summ_route)\n",
    "\n",
    "#1way lstm\n",
    "#text_route = LSTM(lstm_output_size)(text_route)\n",
    "summ_route = LSTM(lstm_output_size)(summ_route)\n",
    "\n",
    "#negate results\n",
    "#merged = Lambda(lambda x: -1*x)(merged)\n",
    "\n",
    "#add p_n constant\n",
    "#merged = Lambda(lambda x: x + c)(merged)\n",
    "\n",
    "#output\n",
    "output = Dense(1, activation='sigmoid')(summ_route)\n",
    "\n",
    "#define model\n",
    "model = Model(inputs=[text_input, summ_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 80, 100)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 80, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 76, 64)            32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 19, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 70)                37800     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 71        \n",
      "=================================================================\n",
      "Total params: 69,935\n",
      "Trainable params: 69,935\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17944/17944 [==============================] - 1458s 81ms/step - loss: 0.4815 - acc: 0.7637 - val_loss: 0.4515 - val_acc: 0.7873\n",
      "Epoch 2/20\n",
      "17944/17944 [==============================] - 730s 41ms/step - loss: 0.4418 - acc: 0.7935 - val_loss: 0.4368 - val_acc: 0.7954\n",
      "Epoch 3/20\n",
      "17944/17944 [==============================] - 743s 41ms/step - loss: 0.4346 - acc: 0.7980 - val_loss: 0.4311 - val_acc: 0.8009\n",
      "Epoch 4/20\n",
      "17944/17944 [==============================] - 744s 41ms/step - loss: 0.4310 - acc: 0.8001 - val_loss: 0.4374 - val_acc: 0.7966\n",
      "Epoch 5/20\n",
      "17944/17944 [==============================] - 750s 42ms/step - loss: 0.4285 - acc: 0.8020 - val_loss: 0.4366 - val_acc: 0.7967\n",
      "Epoch 6/20\n",
      "17944/17944 [==============================] - 747s 42ms/step - loss: 0.4268 - acc: 0.8031 - val_loss: 0.4334 - val_acc: 0.7984\n",
      "Epoch 7/20\n",
      "17944/17944 [==============================] - 764s 43ms/step - loss: 0.4254 - acc: 0.8038 - val_loss: 0.4328 - val_acc: 0.7978\n",
      "Epoch 8/20\n",
      "17944/17944 [==============================] - 767s 43ms/step - loss: 0.4242 - acc: 0.8043 - val_loss: 0.4314 - val_acc: 0.7987\n",
      "Epoch 9/20\n",
      "17944/17944 [==============================] - 763s 43ms/step - loss: 0.4235 - acc: 0.8051 - val_loss: 0.4345 - val_acc: 0.7974\n",
      "Epoch 10/20\n",
      "17944/17944 [==============================] - 766s 43ms/step - loss: 0.4227 - acc: 0.8056 - val_loss: 0.4313 - val_acc: 0.7996\n",
      "Epoch 11/20\n",
      "17944/17944 [==============================] - 748s 42ms/step - loss: 0.4221 - acc: 0.8060 - val_loss: 0.4353 - val_acc: 0.7968\n",
      "Epoch 12/20\n",
      "17944/17944 [==============================] - 758s 42ms/step - loss: 0.4216 - acc: 0.8065 - val_loss: 0.4365 - val_acc: 0.7979\n",
      "Epoch 13/20\n",
      "17944/17944 [==============================] - 764s 43ms/step - loss: 0.4210 - acc: 0.8068 - val_loss: 0.4324 - val_acc: 0.7991\n",
      "Epoch 14/20\n",
      "17944/17944 [==============================] - 755s 42ms/step - loss: 0.4215 - acc: 0.8065 - val_loss: 0.4314 - val_acc: 0.8000\n",
      "Epoch 15/20\n",
      "17944/17944 [==============================] - 782s 44ms/step - loss: 0.4207 - acc: 0.8070 - val_loss: 0.4286 - val_acc: 0.8014\n",
      "Epoch 16/20\n",
      "17944/17944 [==============================] - 759s 42ms/step - loss: 0.4206 - acc: 0.8068 - val_loss: 0.4291 - val_acc: 0.8010\n",
      "Epoch 17/20\n",
      "17944/17944 [==============================] - 755s 42ms/step - loss: 0.4206 - acc: 0.8072 - val_loss: 0.4281 - val_acc: 0.8014\n",
      "Epoch 18/20\n",
      "17944/17944 [==============================] - 773s 43ms/step - loss: 0.4203 - acc: 0.8071 - val_loss: 0.4287 - val_acc: 0.8002\n",
      "Epoch 19/20\n",
      "17944/17944 [==============================] - 789s 44ms/step - loss: 0.4206 - acc: 0.8070 - val_loss: 0.4290 - val_acc: 0.8014\n",
      "Epoch 20/20\n",
      "17944/17944 [==============================] - 803s 45ms/step - loss: 0.4198 - acc: 0.8077 - val_loss: 0.4325 - val_acc: 0.7993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2a395a66d8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#callbacks\n",
    "class BatchHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accs.append(logs.get('acc'))\n",
    "        \n",
    "history = BatchHistory()\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True)\n",
    "modelcheckpoint = ModelCheckpoint('best.h5', monitor='val_loss', verbose=0, save_best_only=True, mode='min', period=1)\n",
    "\n",
    "#batch generator parameters\n",
    "params = {'dim': [(maxlen_text,embedding_size),(maxlen_summ,embedding_size)],\n",
    "          'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "         'tokenizer':tokenizer,\n",
    "         'embedding_matrix':embedding_matrix,\n",
    "         'maxlen_text':maxlen_text,\n",
    "         'maxlen_summ':maxlen_summ,\n",
    "         'data_dir':data_dir,\n",
    "         'sample_info':sample_info}\n",
    "\n",
    "#generators\n",
    "training_generator = ContAllGenerator(partition['train'], labels, **params)\n",
    "validation_generator = ContAllGenerator(partition['validation'], labels, **params)\n",
    "\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=5,\n",
    "                   epochs=epochs,\n",
    "                   callbacks=[tensorboard, modelcheckpoint, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('losses.pickle', 'wb') as handle: pickle.dump(history.losses, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('accs.pickle', 'wb') as handle: pickle.dump(history.accs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
