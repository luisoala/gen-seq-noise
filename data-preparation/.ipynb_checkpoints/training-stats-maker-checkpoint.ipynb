{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cell-width control\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twdisc-embeddedinputmodel_class = \"model_class_5ws\"\n",
    "model_num = \"model_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donald/anaconda3/envs/ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#packages\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "#utils\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import struct\n",
    "import time\n",
    "from noise import *\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Concatenate, Dot, Embedding, LSTM, Conv1D, MaxPooling1D, Input, Lambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(47)\n",
    "\n",
    "def get_data(filename, nc_dist, replace, corr_sample, separate, band_width, noise_candidate_path, dgp,\n",
    "            GLOVE_DIR, val_share, maxlen_text, maxlen_summ,embedding_size,load_tok, tok_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        filename (string): path to data file holding clean datapoints, i.e. clean (text, summ) pairs\n",
    "        nc_dist ((float, float)): (clean_ratio, noise_ratio) tuple describing the desired noise-clean \n",
    "                                    distribution in the output dataset, sum(nc_dist) = 1\n",
    "        replace (bool): whether or not to sample with replacement from clean\n",
    "        corr_sample (bool): whether to compare some of the orig summs and the noise summs to check for\n",
    "                            correspondence\n",
    "        separate (bool): whether to generate noise from a set of texts that is disjoint from the clean data\n",
    "        band_width (int): number of outputs of G per text\n",
    "        noise_candidate_path (string): path where the generated noise files are stored\n",
    "        dgp (string): which DGP is to be used for noise, e.g. \"generator\" or \"random\"\n",
    "        load_tok (bool): whether to load tokenizer or train from scratch\n",
    "        tok_path (string): path to stored tokenizer object\n",
    "        GLOVE_DIR (string): path to glove folder\n",
    "        val_share (float): share (< 1) of data that is to be used for validation\n",
    "        maxlen_text (int): max length of text after which to cut text\n",
    "        maxlen_summ (int): max length of summ after which to cut summ\n",
    "    Returns:\n",
    "        data ((numpy array, numpy array)): (clean, noise) tuple of Nx2 arrays of (text, summ) datapoints \n",
    "                                            with noise\n",
    "    \"\"\"\n",
    "    if dgp == \"generator\":\n",
    "        #get all the bad indices\n",
    "        print('Get all the bad indices...')\n",
    "        alarm_log = {}\n",
    "        alarm_count = 0\n",
    "        count = 4\n",
    "        filenum_old = 'XXXXXX'\n",
    "        name_old = 'XXXXXX'\n",
    "        for name_new in sorted(os.listdir(noise_candidates_path)):\n",
    "            filenum_new = name_new[0:6]\n",
    "            if filenum_new == filenum_old:\n",
    "                count += 1\n",
    "            if filenum_new != filenum_old:\n",
    "                if count != 4:\n",
    "                    alarm_count += 1\n",
    "                    alarm_log[name_old] = count #collect all the bad keys with count\n",
    "                count = 1\n",
    "            filenum_old = filenum_new\n",
    "            name_old = name_new\n",
    "        all_bad_indices = [int(filename[0:6]) for filename in alarm_log.keys()] #now have all the bad indices\n",
    "        print('...done!')\n",
    "\n",
    "        #read in clean data\n",
    "        print('Reading clean data...')\n",
    "        text_summ_pairs = []\n",
    "        with open(filename, 'r') as data:\n",
    "            text = data.readline()\n",
    "            summ = data.readline()\n",
    "            while summ:\n",
    "                if len(text) > 2 and len(summ) > 2:\n",
    "                    text_summ_pairs.append([text[0:-1], summ[0:-1]])\n",
    "                text = data.readline()\n",
    "                summ = data.readline()\n",
    "        clean_2d = numpy.array(text_summ_pairs, dtype=object)\n",
    "        print('...done!')\n",
    "\n",
    "\n",
    "        #remove bad indices\n",
    "        print('Remove bad indices from clean data...')\n",
    "        mask = numpy.ones(clean_2d.shape[0], dtype='bool')\n",
    "        mask[all_bad_indices] = False\n",
    "        clean_2d = clean_2d[mask]\n",
    "        N_clean = clean_2d.shape[0]\n",
    "        print('...done!')\n",
    "\n",
    "        #pick indices of noise\n",
    "        print('Pick noise indices...')\n",
    "        clean_ratio, noise_ratio = nc_dist\n",
    "        if separate: #check whether to generate noise from texts disjoint from clean data\n",
    "            N_noise = int((N_clean*noise_ratio)) #calculate N_noise\n",
    "            #below make sure we have a separate set of indices for noise which we can delete later for the separate\n",
    "            #guarantee\n",
    "            noise_separate_indices = numpy.random.choice(N_clean, size = N_noise, replace=False)\n",
    "            noise_index_pool = numpy.copy(noise_separate_indices)\n",
    "            for i in range(1, band_width):\n",
    "                noise_index_pool = numpy.concatenate((noise_index_pool, noise_separate_indices + i))\n",
    "            assert noise_index_pool.shape[0] == N_noise*band_width, \"noise index pool smaller than expected\"\n",
    "            assert abs(((N_clean - N_noise)/N_clean) - clean_ratio) < 0.0001 \\\n",
    "            and abs((N_noise/N_clean) - noise_ratio) < 0.0001 \\\n",
    "            ,\"Something is wrong with N_noise\" #check that you calculated N_noise correctly\n",
    "        else:\n",
    "            N_noise = int((N_clean - N_clean*clean_ratio)/clean_ratio) #calculate N_noise\n",
    "            noise_index_pool = numpy.arange(N_clean*band_width)\n",
    "            assert abs(N_clean/(N_clean + N_noise) - clean_ratio) < 0.0001 \\\n",
    "                and abs(N_noise/(N_clean + N_noise) - noise_ratio) < 0.0001 \\\n",
    "                ,\"Something is wrong with N_noise\" #check that you calculated N_noise correctly\n",
    "        noise_summ_indices = numpy.random.choice(noise_index_pool, size=N_noise, replace=replace) #get indices \\\n",
    "            #in the range of N_clean*(band_width of generator run, i.e. number of outputs of G per text)\n",
    "        assert N_noise == len(noise_summ_indices), \"N_noise and len(selected_indices do not match)\"\n",
    "        print('...done!')\n",
    "\n",
    "        #read in candidate noise points\n",
    "        print('Read in candidate noise points...')\n",
    "        candidate_noise = []\n",
    "        for filename in sorted(os.listdir(noise_candidates_path)):\n",
    "            if int(filename[0:6]) not in all_bad_indices:\n",
    "                with open(noise_candidates_path+filename, 'r') as file:\n",
    "                    candidate_noise.append(file.read().replace('\\n', ' ')) #read file, trim \\n and add to cand. list\n",
    "        assert len(candidate_noise) == band_width*clean_2d.shape[0], \"less candidates than expected\"\n",
    "        print('...done!')\n",
    "\n",
    "        #preprocess clean data, i.e. remove <s> and </s>\n",
    "        print('Preprocess clean data, i.e. remove <s> and </s>...')\n",
    "        for i in range(N_clean):\n",
    "            clean_2d[i,1] = clean_2d[i,1].replace('<s> ', '')\n",
    "            clean_2d[i,1] = clean_2d[i,1].replace(' </s>', '')\n",
    "        print('...done!')\n",
    "\n",
    "        if corr_sample: #take some samples to sanity check that refs and generated summs correspond\n",
    "            print('Sanity check some examples..')\n",
    "            idx = numpy.random.choice(N_noise, size=10, replace=False)\n",
    "            for i in idx:\n",
    "                print('### '+str(i)+' ###')\n",
    "                print('reference summary '+':\\n'+clean_2d[(noise_summ_indices[i] // 4),1])\n",
    "                print('generated summary'+':\\n'+candidate_noise[i])\n",
    "            print('...done!')\n",
    "\n",
    "        #put data together\n",
    "        print('Put data together...')\n",
    "        noise_texts = clean_2d[(noise_summ_indices // 4),0]\n",
    "        print('noise_texts.shape[0]', noise_texts.shape[0])\n",
    "        noise_summs = numpy.array(candidate_noise)[noise_summ_indices]\n",
    "        print('noise_summs.shape[0]', noise_summs.shape[0])\n",
    "        noise_2d = numpy.stack((noise_texts,noise_summs), axis=-1)\n",
    "        assert noise_2d.shape[0] == N_noise and noise_2d.shape[1] == 2, \"the noise_2d shape does not check out\"\n",
    "        print('...done!')\n",
    "\n",
    "        #remove noise source texts from clean data if separate was selected\n",
    "        if separate: #delete inputs for noise from clean data\n",
    "            print('prior', clean_2d.shape)\n",
    "            clean_2d = numpy.delete(clean_2d, noise_separate_indices, axis=0)\n",
    "            print('after', clean_2d.shape)\n",
    "            #TODO: this might skrew up the dist in the case were an index appears multiple times in sel_ind\n",
    "\n",
    "        return clean_2d, noise_2d\n",
    "    \n",
    "    elif dgp == 'random':\n",
    "        #read in clean data\n",
    "        print('Reading clean data...')\n",
    "        text_summ_pairs = []\n",
    "        with open(filename, 'r') as data:\n",
    "            text = data.readline()\n",
    "            summ = data.readline()\n",
    "            while summ:\n",
    "                if len(text) > 2 and len(summ) > 2:\n",
    "                    text_summ_pairs.append([text[0:-1], summ[0:-1]])\n",
    "                text = data.readline()\n",
    "                summ = data.readline()\n",
    "        clean_2d = numpy.array(text_summ_pairs, dtype=object)\n",
    "        print('...done!')\n",
    "        \n",
    "        #create and fit tokenizer\n",
    "        print('Create and fit tokenizer...')\n",
    "        \n",
    "        if load_tok:\n",
    "            with open(tok_path, 'rb') as handle:\n",
    "                tokenizer = pickle.load(handle)\n",
    "        else:\n",
    "            tokenizer = Tokenizer(num_words=max_features,\n",
    "                                       filters='#$%&()*+-/:;<=>@[\\\\]^_{|}~\\t\\n',\n",
    "                                       lower=True,\n",
    "                                       split=\" \",\n",
    "                                       char_level=False)\n",
    "            tokenizer.fit_on_texts(numpy.append(clean_2d[:,0], clean_2d[:,1]))\n",
    "        print('...done!')\n",
    "        \n",
    "        \n",
    "        #read in candidate noise points\n",
    "        #print('Read in candidate noise points...')\n",
    "        #fractions = {\"switch-pairs\":0.25,\"sentence-switch-entire-bank\":0.25,\\\n",
    "                     #\"sentence-switch-same-text-bank\":0.25,\"word-switch-entire-bank\":0.25}\n",
    "        #clean_2d, noise_2d = noise_randomDGP(clean_2d, fractions, separate, nc_dist)\n",
    "        #print('...done!')\n",
    "        \n",
    "        #preprocess clean data, i.e. remove <s> and </s>\n",
    "        print('Preprocess clean data, i.e. remove <s> and </s>...')\n",
    "        for i in range(clean_2d.shape[0]):\n",
    "            clean_2d[i,1] = clean_2d[i,1].replace('<s> ', '')\n",
    "            clean_2d[i,1] = clean_2d[i,1].replace(' </s>', '')\n",
    "        print('...done!')\n",
    "        \n",
    "        #make embedding matrix\n",
    "        print('Make embedding matrix...')\n",
    "        embeddings_index = {}\n",
    "        f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = numpy.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        \n",
    "        word_index = tokenizer.word_index\n",
    "        embedding_matrix = numpy.zeros((len(word_index) + 1, embedding_size))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        print('...done!')\n",
    "        \n",
    "        #split data into train & val\n",
    "        print('split data into train & val...')\n",
    "            #split texts and summs\n",
    "        texts = clean_2d[:,0]\n",
    "        summs = clean_2d[:,1]\n",
    "\n",
    "            #get targets\n",
    "        N_clean = clean_2d.shape[0]\n",
    "        #N_noise = noise_2d.shape[0]\n",
    "        targets = [1]*N_clean\n",
    "\n",
    "            #permute targets and data in the same way\n",
    "        #indices = numpy.random.choice(N_clean+N_noise, size=N_clean+N_noise, replace=False)\n",
    "        #assert len(indices) == N_clean+N_noise, \"indices are less N_clean + N_noise\"\n",
    "        #texts = texts[indices]\n",
    "        #summs = summs[indices]\n",
    "        #targets = targets[indices]\n",
    "\n",
    "            #split data into train and test\n",
    "        #split = int((N_clean+N_noise)*val_share)\n",
    "        #texts_train = texts[split:]\n",
    "        #summs_train = summs[split:]\n",
    "        #targets_train = targets[split:]\n",
    "        #texts_val = texts[:split]\n",
    "        #summs_val = summs[:split]\n",
    "        #targets_val = targets[:split]\n",
    "        #print('train dist: ', numpy.mean(targets_train)) #just checking what the dists are after permute\n",
    "        #print('val dist: ', numpy.mean(targets_val)) #just checking what the dists are after permute\n",
    "        #print('...done!')\n",
    "        \n",
    "        #sequentialize data\n",
    "        #print('sequentialize data...')\n",
    "        #texts_train_seq = tokenizer.texts_to_sequences(texts_train)\n",
    "        #summs_train_seq = tokenizer.texts_to_sequences(summs_train)\n",
    "        #texts_val_seq = tokenizer.texts_to_sequences(texts_val)\n",
    "        #summs_val_seq = tokenizer.texts_to_sequences(summs_val)\n",
    "\n",
    "            #pad data\n",
    "        #texts_train_seq = sequence.pad_sequences(texts_train_seq, maxlen=maxlen_text)\n",
    "        #summs_train_seq = sequence.pad_sequences(summs_train_seq, maxlen=maxlen_summ)\n",
    "        #texts_val_seq = sequence.pad_sequences(texts_val_seq, maxlen=maxlen_text)\n",
    "        #summs_val_seq = sequence.pad_sequences(summs_val_seq, maxlen=maxlen_summ)\n",
    "        #print('...done!')\n",
    "        \n",
    "        partition = {}\n",
    "        partition['train'] = []\n",
    "        partition['validation'] = []\n",
    "        labels = {}\n",
    "        id_counter = 1\n",
    "        #go through train data to write to: embed, write to file with id, add id to train partition, add id and label to label dict\n",
    "        print('write string data to file...')\n",
    "        for i in range(N_clean):\n",
    "            store_string = texts[i]+'\\n'+summs[i]\n",
    "            id_name = 'id-'+str(id_counter)\n",
    "            with open('/media/oala/4TB/data-test/'+id_name, 'w') as file:\n",
    "                file.write(store_string)\n",
    "            partition['train'] += [id_name]\n",
    "            labels[id_name] = targets[i]\n",
    "            id_counter += 1\n",
    "        print('...done!')\n",
    "        \n",
    "        \n",
    "        #store embedding matrix\n",
    "        print('store embedding matrix...')\n",
    "        numpy.save('/media/oala/4TB/data-test/'+'embedding-matrix', embedding_matrix)\n",
    "        print('...done!')\n",
    "        \n",
    "        #store label dict\n",
    "        print('store label dict...')\n",
    "        with open('/media/oala/4TB/data-test/'+'labels.pickle', 'wb') as handle:\n",
    "            pickle.dump(labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('...done!')\n",
    "        \n",
    "        #store tokenizer\n",
    "        print('store tokenizer...')\n",
    "        with open('/media/oala/4TB/data-test/'+'tokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('...done!')\n",
    "        \n",
    "        #store partition dict\n",
    "        print('partition dict...')\n",
    "        with open('/media/oala/4TB/data-test/'+'partition.pickle', 'wb') as handle:\n",
    "            pickle.dump(partition, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('...done!')\n",
    "        \n",
    "        #return clean_2d, noise_2d\n",
    "        return partition, labels, tokenizer, embedding_matrix\n",
    "    else:\n",
    "        print('error: no valid DGP was selected')\n",
    "\n",
    "def prep_data(clean_2d, noise_2d, max_features, val_share, maxlen_text, maxlen_summ,\n",
    "              load_tok=False, tok_path=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        clean_2d (numpy array): Nx2 array of text summ tuples with clean points\n",
    "        noise_2d (numpy array): Nx2 array of text summ tuples with noise points\n",
    "        max_features (int): max number of words for tokenizer\n",
    "        val_share (float): share (< 1) of data that is to be used for validation\n",
    "        load_tok (bool): whether to load tokenizer or train from scratch\n",
    "        tok_path (string): path to stored tokenizer object\n",
    "        maxlen_text (int): max length of text after which to cut text\n",
    "        maxlen_summ (int): max length of summ after which to cut summ\n",
    "    Returns:\n",
    "        texts_train_seq (array): (N_clean+N_noise)*(1-val_share)xmaxlen_text array of seq textgot multiple values for argument\n",
    "        summs_train_seq (array): (N_clean+N_noise)*(1-val_share)xmaxlen_summ array of seq summ\n",
    "        text_val_seq (array): (N_clean+N_noise)*(val_share)xmaxlen_text array of seq text\n",
    "        summs_val_seq (array): (N_clean+N_noise)*(val_share)xmaxlen_summ array of seq summ\n",
    "        tokenizer (tokenizer object): tokenizer object\n",
    "    \"\"\"\n",
    "    #split texts and summs\n",
    "    texts = numpy.append(clean_2d[:,0], noise_2d[:,0])\n",
    "    summs = numpy.append(clean_2d[:,1], noise_2d[:,1])\n",
    "    \n",
    "    #get targets\n",
    "    N_clean = clean_2d.shape[0]\n",
    "    N_noise = noise_2d.shape[0]\n",
    "    targets = numpy.append([0]*N_clean, [1]*N_noise)\n",
    "    \n",
    "    #permute targets and data in the same way\n",
    "    indices = numpy.random.choice(N_clean+N_noise, size=N_clean+N_noise, replace=False)\n",
    "    assert len(indices) == N_clean+N_noise, \"indices are less N_clean + N_noise\"\n",
    "    texts = texts[indices]\n",
    "    summs = summs[indices]\n",
    "    targets = targets[indices]\n",
    "    \n",
    "    #split data into train and test\n",
    "    split = int((N_clean+N_noise)*val_share)\n",
    "    texts_train = texts[split:]\n",
    "    summs_train = summs[split:]\n",
    "    targets_train = targets[split:]\n",
    "    texts_val = texts[:split]\n",
    "    summs_val = summs[:split]\n",
    "    targets_val = targets[:split]\n",
    "    print('train dist: ', numpy.mean(targets_train)) #just checking what the dists are after permute\n",
    "    print('val dist: ', numpy.mean(targets_val)) #just checking what the dists are after permute\n",
    "    \n",
    "    #train tokenizer\n",
    "    if load_tok:\n",
    "        with open(tok_path, 'rb') as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(num_words=max_features,\n",
    "                                   filters='#$%&()*+-/:;<=>@[\\\\]^_{|}~\\t\\n',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False)\n",
    "        tokenizer.fit_on_texts(numpy.append(texts_train, summs_train))\n",
    "    \n",
    "    #sequentialize data\n",
    "    texts_train_seq = tokenizer.texts_to_sequences(texts_train)\n",
    "    summs_train_seq = tokenizer.texts_to_sequences(summs_train)\n",
    "    texts_val_seq = tokenizer.texts_to_sequences(texts_val)\n",
    "    summs_val_seq = tokenizer.texts_to_sequences(summs_val)\n",
    "    \n",
    "    #pad data\n",
    "    texts_train_seq = sequence.pad_sequences(texts_train_seq, maxlen=maxlen_text)\n",
    "    summs_train_seq = sequence.pad_sequences(summs_train_seq, maxlen=maxlen_summ)\n",
    "    texts_val_seq = sequence.pad_sequences(texts_val_seq, maxlen=maxlen_text)\n",
    "    summs_val_seq = sequence.pad_sequences(summs_val_seq, maxlen=maxlen_summ)\n",
    "    \n",
    "    return texts_train_seq, summs_train_seq, targets_train, texts_val_seq, summs_val_seq, targets_val, tokenizer\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, tokenizer, embedding_matrix, maxlen_text, maxlen_summ, batch_size=32, dim=(1,1), n_channels=1,\n",
    "                 n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.maxlen_text = maxlen_text\n",
    "        self.maxlen_summ = maxlen_summ\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X_one = np.empty((self.batch_size, *self.dim[0]))\n",
    "        X_two = np.empty((self.batch_size, *self.dim[1]))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            \n",
    "            with open('/media/oala/4TB/data-test/' + ID, 'r') as file:\n",
    "                data_point = file.read()\n",
    "                #text = data.readline()\n",
    "                #summ = data.readline()\n",
    "            \n",
    "            \n",
    "            \n",
    "            #data_point = np.load('/media/oala/4TB/data-test/' + ID)\n",
    "            text, summ = data_point.split('\\n')\n",
    "            #print('text',text)\n",
    "            #print('summ',summ)\n",
    "            \n",
    "            text = self.tokenizer.texts_to_sequences(numpy.array([text], dtype=object))\n",
    "            summ = self.tokenizer.texts_to_sequences(numpy.array([summ], dtype=object))\n",
    "            \n",
    "            text = sequence.pad_sequences(text, maxlen=self.maxlen_text, truncating = 'post', padding = 'pre')\n",
    "            summ = sequence.pad_sequences(summ, maxlen=self.maxlen_summ, truncating = 'post', padding = 'post')\n",
    "            \n",
    "            #print('X_one', X_one.shape)\n",
    "            #print('X_one[i]',X_one[i].shape)\n",
    "            #print('embedding_matrix[text[numpy.newaxis,:,:]]',embedding_matrix[text[numpy.newaxis,:,:]].shape)\n",
    "            #print('text', text.shape)\n",
    "            #print('summ', summ.shape)\n",
    "            #print('embedding_matrix', self.embedding_matrix.shape)\n",
    "            X_one[i] = self.embedding_matrix[text[numpy.newaxis,:,:]]\n",
    "            X_two[i] = self.embedding_matrix[summ[numpy.newaxis,:,:]]\n",
    "            \n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return [X_one, X_two], y#keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "\n",
    "def get_embedded_input(embedding_matrix, word_index, embedding_size, maxlen_text, maxlen_summ,text_input, summ_input):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        embedding_matrix (numpy.array): array where 0-axis indices map to word-vectors\n",
    "        word_index (dict): index to word dict\n",
    "        embedding_size (int): dimension of embeddings\n",
    "        maxlen_text (int): maximum length of text\n",
    "        maxlen_summ (int): maximum length of summaries\n",
    "        text_input (numpy.array): Nxmaxlen_text index representation of texts\n",
    "        summ_input (numpy.array): Nxmaxlen_summ index representation of summs\n",
    "    \n",
    "    Returns:\n",
    "        text_embed (numpy.array): N x maxlen_text x embed_dim rep of texts\n",
    "        summ_embed (numpy.array): N x maxlen_summ x embed_dim rep of summs\n",
    "    \n",
    "    \"\"\"\n",
    "    #define custom embeddings layer\n",
    "    embedding_layer_text = Embedding(len(word_index) + 1,\n",
    "                                embedding_size,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length= maxlen_text,\n",
    "                                trainable=False)\n",
    "\n",
    "    embedding_layer_summ = Embedding(len(word_index) + 1,\n",
    "                                embedding_size,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length= maxlen_summ,\n",
    "                                trainable=False)\n",
    "    \n",
    "    #2way input\n",
    "    text_input = Input(shape=(maxlen_text,), dtype='int32')\n",
    "    summ_input = Input(shape=(maxlen_summ,), dtype='int32')\n",
    "\n",
    "    #2way embeddings\n",
    "    text_route = embedding_layer_text(text_input)\n",
    "    summ_route = embedding_layer_summ(summ_input)\n",
    "    \n",
    "    text_route = Activation('linear')(text_route)\n",
    "    summ_route = Activation('linear')(summ_route)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #put together model\n",
    "    model = Model(inputs=[text_input, summ_input], outputs=[text_route, summ_route])\n",
    "    #model.compile(loss='binary_crossentropy',\n",
    "                  #optimizer='adam',\n",
    "                  #metrics=['accuracy'])\n",
    "    \n",
    "    #text_embed, summ_embed = model.predict([text_input, summ_input])\n",
    "    return model.predict([text_input, summ_input]) #text_embed, summ_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading clean data...\n",
      "...done!\n",
      "Preprocess clean data, i.e. remove <s> and </s>...\n",
      "...done!\n",
      "Create and fit tokenizer...\n",
      "...done!\n",
      "Make embedding matrix...\n",
      "...done!\n",
      "split data into train & val...\n",
      "sequentialize data...\n"
     ]
    }
   ],
   "source": [
    "#read in clean data\n",
    "print('Reading clean data...')\n",
    "text_summ_pairs = []\n",
    "with open(filename, 'r') as data:\n",
    "    text = data.readline()\n",
    "    summ = data.readline()\n",
    "    while summ:\n",
    "        if len(text) > 2 and len(summ) > 2:\n",
    "            text_summ_pairs.append([text[0:-1], summ[0:-1]])\n",
    "        text = data.readline()\n",
    "        summ = data.readline()\n",
    "clean_2d = numpy.array(text_summ_pairs, dtype=object)\n",
    "print('...done!')\n",
    "\n",
    "#preprocess clean data, i.e. remove <s> and </s>\n",
    "print('Preprocess clean data, i.e. remove <s> and </s>...')\n",
    "for i in range(clean_2d.shape[0]):\n",
    "    clean_2d[i,1] = clean_2d[i,1].replace('<s> ', '')\n",
    "    clean_2d[i,1] = clean_2d[i,1].replace(' </s>', '')\n",
    "print('...done!')\n",
    "\n",
    "#create and fit tokenizer\n",
    "print('Create and fit tokenizer...')\n",
    "tokenizer = Tokenizer(num_words=max_features,\n",
    "                           filters='#$%&()*+-/:;<=>@[\\\\]^_{|}~\\t\\n',\n",
    "                           lower=True,\n",
    "                           split=\" \",\n",
    "                           char_level=False)\n",
    "tokenizer.fit_on_texts(numpy.append(clean_2d[:,0], clean_2d[:,1]))\n",
    "print('...done!')\n",
    "\n",
    "\n",
    "#read in candidate noise points\n",
    "#print('Read in candidate noise points...')\n",
    "#fractions = {\"switch-pairs\":0.25,\"sentence-switch-entire-bank\":0.25,\\\n",
    "             #\"sentence-switch-same-text-bank\":0.25,\"word-switch-entire-bank\":0.25}\n",
    "#clean_2d, noise_2d = noise_randomDGP(clean_2d, fractions, separate, nc_dist)\n",
    "#print('...done!')\n",
    "\n",
    "#make embedding matrix\n",
    "print('Make embedding matrix...')\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = numpy.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('...done!')\n",
    "\n",
    "#split data into train & val\n",
    "print('split data into train & val...')\n",
    "    #split texts and summs\n",
    "texts = clean_2d[:,0]\n",
    "summs = clean_2d[:,1]\n",
    "\n",
    "    #get targets\n",
    "N_clean = clean_2d.shape[0]\n",
    "#N_noise = noise_2d.shape[0]\n",
    "targets = [1]*N_clean\n",
    "\n",
    "    #permute targets and data in the same way\n",
    "#indices = numpy.random.choice(N_clean+N_noise, size=N_clean+N_noise, replace=False)\n",
    "#assert len(indices) == N_clean+N_noise, \"indices are less N_clean + N_noise\"\n",
    "#texts = texts[indices]\n",
    "#summs = summs[indices]\n",
    "#targets = targets[indices]\n",
    "\n",
    "    #split data into train and test\n",
    "#split = int((N_clean+N_noise)*val_share)\n",
    "#texts_train = texts[split:]\n",
    "#summs_train = summs[split:]\n",
    "#targets_train = targets[split:]\n",
    "#texts_val = texts[:split]\n",
    "#summs_val = summs[:split]\n",
    "#targets_val = targets[:split]\n",
    "#print('train dist: ', numpy.mean(targets_train)) #just checking what the dists are after permute\n",
    "#print('val dist: ', numpy.mean(targets_val)) #just checking what the dists are after permute\n",
    "#print('...done!')\n",
    "\n",
    "#sequentialize data\n",
    "print('sequentialize data...')\n",
    "texts_train_seq = tokenizer.texts_to_sequences(texts)\n",
    "summs_train_seq = tokenizer.texts_to_sequences(summs)\n",
    "#texts_val_seq = tokenizer.texts_to_sequences(texts_val)\n",
    "#summs_val_seq = tokenizer.texts_to_sequences(summs_val)\n",
    "\n",
    "    #pad data\n",
    "texts_train_seq = sequence.pad_sequences(texts_train_seq, maxlen=maxlen_text)\n",
    "summs_train_seq = sequence.pad_sequences(summs_train_seq, maxlen=maxlen_summ)\n",
    "#texts_val_seq = sequence.pad_sequences(texts_val_seq, maxlen=maxlen_text)\n",
    "#summs_val_seq = sequence.pad_sequences(summs_val_seq, maxlen=maxlen_summ)\n",
    "#print('...done!')\n",
    "\n",
    "all_seq = numpy.concatenate((texts_train_seq, summs_train_seq), axis=1)\n",
    "N,D = all_seq.shape\n",
    "\n",
    "all_seq_embedded = numpy.zeros((N,D,embedding_size))\n",
    "all_seq_embedded = embedding_matrix[all_seq]\n",
    "\n",
    "all_seq_embedded = numpy.reshape(all_seq_embedded,(N,D*embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.save('/mnt/disks/500gb/stats-and-meta-data/400000/embedding_matrix',embedding_matrix)\n",
    "with open('/mnt/disks/500gb/stats-and-meta-data/400000/tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(287113, 48000)\n"
     ]
    }
   ],
   "source": [
    "print(all_seq_embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all_seq_embedded in ten portions\n",
    "for i in range(10):\n",
    "    numpy.save('/mnt/disks/500gb/stats-and-meta-data/400000/training-chunks/%d'%i,\n",
    "               all_seq_embedded[:,4800*i:4800*i + 4800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text and summ stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537048, 100)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#load and collect statistics from portions\n",
    "mean_list = []\n",
    "std_list = []\n",
    "mini_list = []\n",
    "maxi_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    a = numpy.load('/mnt/disks/500gb/stats-and-meta-data/400000/training-chunks/%d.npy'%i)\n",
    "    mean_list.append(numpy.mean(a,axis=0, keepdims=True))\n",
    "    std_list.append(numpy.var(a,axis=0, keepdims=True)**0.5)\n",
    "    mini_list.append(numpy.amin(a, axis=0, keepdims=True))\n",
    "    maxi_list.append(numpy.amax(a, axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate portion results\n",
    "mean = numpy.concatenate(mean_list,axis=1)\n",
    "std = numpy.concatenate(std_list, axis=1)\n",
    "mini = numpy.concatenate(mini_list, axis=1)\n",
    "maxi = numpy.concatenate(maxi_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print to file\n",
    "numpy.save('/mnt/disks/500gb/stats-and-meta-data/400000/training-stats-all/mean',mean[0])\n",
    "numpy.save('/mnt/disks/500gb/stats-and-meta-data/400000/training-stats-all/std',std[0])\n",
    "numpy.save('/mnt/disks/500gb/stats-and-meta-data/400000/training-stats-all/mini',mini[0])\n",
    "numpy.save('/mnt/disks/500gb/stats-and-meta-data/400000/training-stats-all/maxi',maxi[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000,)\n",
      "(48000,)\n",
      "(48000,)\n",
      "(48000,)\n"
     ]
    }
   ],
   "source": [
    "print(mean[0].shape)\n",
    "print(std[0].shape)\n",
    "print(mini[0].shape)\n",
    "print(maxi[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only summ stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(287113, 9600)\n",
      "(287113, 8000)\n"
     ]
    }
   ],
   "source": [
    "#load and collect statistics from portions\n",
    "a = numpy.load('/mnt/disks/500gb/stats-and-meta-data/400000/training-chunks/8.npy')\n",
    "b = numpy.load('/mnt/disks/500gb/stats-and-meta-data/400000/training-chunks/9.npy')\n",
    "c = numpy.concatenate((a,b), axis=1)\n",
    "\n",
    "print(c.shape)\n",
    "\n",
    "d = c[:,1600:]\n",
    "\n",
    "print(d.shape)\n",
    "\n",
    "mean = numpy.mean(d,axis=0)\n",
    "std = numpy.var(d,axis=0)**0.5\n",
    "mini = numpy.amin(d, axis=0)\n",
    "maxi = numpy.amax(d, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000,)\n",
      "(8000,)\n",
      "(8000,)\n",
      "(8000,)\n"
     ]
    }
   ],
   "source": [
    "print(mean.shape)\n",
    "print(std.shape)\n",
    "print(mini.shape)\n",
    "print(maxi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print to file\n",
    "numpy.save('/mnt/disks/500gb/stats-and-meta-data/400000/training-stats-summ/mean',mean)\n",
    "numpy.save('/mnt/disks/500gb/stats-and-meta-data/400000/training-stats-summ/std',std)\n",
    "numpy.save('/mnt/disks/500gb/stats-and-meta-data/400000/training-stats-summ/mini',mini)\n",
    "numpy.save('/mnt/disks/500gb/stats-and-meta-data/400000/training-stats-summ/maxi',maxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 400000\n",
    "maxlen_text = 400\n",
    "maxlen_summ = 80\n",
    "embedding_size = 100 #128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 50\n",
    "\n",
    "#Saving?\n",
    "save = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"/home/oala/Documents/MT/data/datasets/finished_files/train.bin\"\n",
    "#noise_candidates_path = '/home/oala/Documents/MT/noising/4-beam-PGC-noise-on-train/pretrained_model_tf1.2.1/decode_train_400maxenc_4beam_35mindec_120maxdec_ckpt-238410/decoded/'\n",
    "filename = \"/home/donald/documents/MT/data/data-essentials-mini/finished_files/train.bin\"\n",
    "noise_candidate_path = ' '\n",
    "nc_dist = (0.5,0.5)\n",
    "replace = False\n",
    "corr_sample = False\n",
    "separate = False\n",
    "band_width = 4\n",
    "dgp = \"random\"\n",
    "#GLOVE_DIR = \"/home/oala/Documents/MT/data/datasets/glove.6B/\"\n",
    "GLOVE_DIR = \"/home/donald/documents/MT/data/data-essentials-mini/glove.6B/\"\n",
    "val_share = 0.2\n",
    "#partition, labels, tokenizer, embedding_matrix = get_data(filename, nc_dist, replace, corr_sample, separate, band_width, noise_candidate_path, dgp,GLOVE_DIR, val_share, maxlen_text, maxlen_summ,embedding_size,load_tok=False, tok_path=None)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-032018f04e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_2d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_2d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_2d' is not defined"
     ]
    }
   ],
   "source": [
    "print(clean_2d.shape)\n",
    "print(noise_2d.shape)\n",
    "print(clean_2d[100])\n",
    "print(noise_2d[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dist:  0.4976857263079153\n",
      "val dist:  0.509257527585562\n"
     ]
    }
   ],
   "source": [
    "val_share = 0.2\n",
    "\n",
    "texts_train, summs_train, targets_train, texts_val, summs_val, targets_val, tokenizer = \\\n",
    "    prep_data(clean_2d, noise_2d, max_features, val_share, maxlen_text, maxlen_summ,\n",
    "              load_tok=False, tok_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"/home/oala/Documents/MT/data/datasets/glove.6B/\"\n",
    "#GLOVE_DIR = \"/home/donald/documents/MT/data/data-essentials-mini/glove.6B/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = numpy.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = numpy.zeros((len(word_index) + 1, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom embeddings layer\n",
    "embedding_layer_text = Embedding(len(word_index) + 1,\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length= maxlen_text,\n",
    "                            trainable=False)\n",
    "\n",
    "embedding_layer_summ = Embedding(len(word_index) + 1,\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length= maxlen_summ,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make input data embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21389, 400)\n",
      "(21389, 80)\n",
      "(21389, 400, 100)\n",
      "(21389, 80, 100)\n",
      "(5347, 400, 100)\n",
      "(5347, 80, 100)\n"
     ]
    }
   ],
   "source": [
    "#texts_train\n",
    "\n",
    "print(texts_train.shape)\n",
    "print(summs_train.shape)\n",
    "\n",
    "N,_ =texts_train.shape\n",
    "\n",
    "texts_train_embedded = numpy.zeros((N,maxlen_text,embedding_size))\n",
    "texts_train_embedded = embedding_matrix[texts_train]\n",
    "summs_train_embedded = numpy.zeros((N,maxlen_summ,embedding_size))\n",
    "summs_train_embedded = embedding_matrix[summs_train]\n",
    "texts_val_embedded = numpy.zeros((N,maxlen_text,embedding_size))\n",
    "texts_val_embedded = embedding_matrix[texts_val]\n",
    "summs_val_embedded = numpy.zeros((N,maxlen_summ,embedding_size))\n",
    "summs_val_embedded = embedding_matrix[summs_val]\n",
    "\n",
    "#print(texts_train_embedded.shape)\n",
    "#print(texts_train[0,10])\n",
    "#inv_map = {v: k for k, v in word_index.items()}\n",
    "#print(inv_map[texts_train[0,10]])\n",
    "#print(embeddings_index[inv_map[texts_train[0,10]]])\n",
    "#print(texts_train_embedded[0,10])\n",
    "print(texts_train_embedded.shape)\n",
    "print(summs_train_embedded.shape)\n",
    "print(texts_val_embedded.shape)\n",
    "print(summs_val_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2way input\n",
    "text_input = Input(shape=(maxlen_text,embedding_size), dtype='float32')\n",
    "summ_input = Input(shape=(maxlen_summ,embedding_size), dtype='float32')\n",
    "\n",
    "#2way dropout\n",
    "text_route = Dropout(0.25)(text_input)\n",
    "summ_route = Dropout(0.25)(summ_input)\n",
    "\n",
    "#2way conv\n",
    "text_route = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(text_route)\n",
    "summ_route = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(summ_route)\n",
    "\n",
    "#2way max pool\n",
    "text_route = MaxPooling1D(pool_size=pool_size)(text_route)\n",
    "summ_route = MaxPooling1D(pool_size=pool_size)(summ_route)\n",
    "\n",
    "#2way lstm\n",
    "text_route = LSTM(lstm_output_size)(text_route)\n",
    "summ_route = LSTM(lstm_output_size)(summ_route)\n",
    "\n",
    "#merge both routes\n",
    "#merged = keras.layers.concatenate((text_route, summ_route), axis=-1)\n",
    "#merged = Concatenate(axis=-1)([text_route, summ_route])\n",
    "merged = Dot(axes=1,normalize=True)([text_route, summ_route])\n",
    "\n",
    "#output\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "#define model\n",
    "model = Model(inputs=[text_input, summ_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9570/9570 [==============================] - 994s 104ms/step - loss: 0.0441 - acc: 1.0000\n",
      "Epoch 2/50\n",
      "9570/9570 [==============================] - 980s 102ms/step - loss: 1.8790e-04 - acc: 1.0000\n",
      "Epoch 3/50\n",
      "9570/9570 [==============================] - 997s 104ms/step - loss: 2.0403e-06 - acc: 1.0000\n",
      "Epoch 4/50\n",
      "1984/9570 [=====>........................] - ETA: 13:04 - loss: 1.1921e-07 - acc: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-68:\n",
      "Process ForkPoolWorker-67:\n",
      "Process ForkPoolWorker-65:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-66:\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 390, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"<ipython-input-36-3ee882b6c2f3>\", line 399, in __getitem__\n",
      "    X, y = self.__data_generation(list_IDs_temp)\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"<ipython-input-36-3ee882b6c2f3>\", line 432, in __data_generation\n",
      "    text = self.tokenizer.texts_to_sequences(numpy.array([text], dtype=object))\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/site-packages/keras/preprocessing/text.py\", line 248, in texts_to_sequences\n",
      "    for vect in self.texts_to_sequences_generator(texts):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/site-packages/keras/preprocessing/text.py\", line 273, in texts_to_sequences_generator\n",
      "    if i is not None:\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/oala/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-00c08e7721e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                    epochs=epochs)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2242\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2243\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2244\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1888\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np = numpy\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Parameters\n",
    "params = {'dim': [(maxlen_text,embedding_size),(maxlen_summ,embedding_size)],\n",
    "          'batch_size': batch_size,\n",
    "          'n_classes': 2,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': False}\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, tokenizer, embedding_matrix, maxlen_text, maxlen_summ, **params)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, tokenizer, embedding_matrix, maxlen_text, maxlen_summ, **params)\n",
    "\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=4,\n",
    "                   epochs=epochs)\n",
    "\n",
    "\n",
    "\n",
    "#print('Train...')\n",
    "#model.fit([texts_train_embedded, summs_train_embedded], targets_train,\n",
    "          #batch_size=batch_size,\n",
    "          #epochs=epochs,\n",
    "          #validation_data=([texts_val_embedded, summs_val_embedded], targets_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string = \"gary d. cohn , president trumps top economic adviser , said on tuesday that he would resign , becoming the latest in a series of high-profile departures from the trump administration . white house officials insisted that there was no single factor behind the departure of mr. cohn , who heads the national economic council . but his decision to leave came as he seemed poised to lose an internal struggle over mr. trumps plan to impose large tariffs on steel and aluminum imports. mr. cohn had warned last week that he might resign if mr. trump followed through with the tariffs, which mr. cohn had lobbied against internally .   gary has been my chief economic adviser and did a superb job in driving our agenda , helping to deliver historic tax cuts and reforms and unleashing the american economy once again ,  mr. trump said in a statement to the new york times .  he is a rare talent , and i thank him for his dedicated service to the american people .   mr. cohn is expected to leave in the coming weeks. he will join a string of recent departures by senior white house officials, including mr. trumps communications director and a powerful staff secretary.  yet the departure of mr. cohn , a free-trade-oriented democrat who fended off a number of nationalist-minded policies during his year in the trump administration , could have a ripple effect on the presidents economic decisions and on the financial industry .  it leaves mr. trump surrounded primarily by advisers with strong protectionist views who advocate the types of aggressive trade measures , like tariffs , that mr. trump campaigned on but that mr. cohn fought inside the white house . mr. cohn was viewed by republican lawmakers as the steady hand who could prevent mr. trump from engaging in activities that could trigger a trade war.  even the mere threat , last august , that mr. cohn might leave sent the financial markets tumbling. on tuesday , mr. cohns announcement rattled markets , and trading in futures pointed to a decline in the united states stock market when it opened on wednesday .  in a statement , mr. cohn said he had been pleased to work on pro-growth economic policies to benefit the american people , in particular the passage of historic tax reform .  white house officials said that mr. cohn was leaving on cordial terms with the president and that they planned to discuss policy even after his departure .  mr. cohns departure comes as the white house has been buffeted by turnover , uncertainty and internal divisions and as the president lashes out at the special counsel investigation that seems to be bearing down on his team .  a host of top aides have been streaming out the white house door or are considering a departure . rob porter , the white house staff secretary and a member of the inner circle , resigned after spousal abuse allegations . hope hicks , the presidents communications director and confidante , announced that she would leave soon . in recent days , the president has lost a speechwriter , an associate attorney general and the north korea negotiator .  others are perpetually seen as on the way out . john f. kelly , the chief of staff , at one point broached resigning over the handling of mr. porters case . lt. gen. h. r. mcmaster , the national security adviser , has been reported to be preparing to leave . and many officials wonder if jared kushner , the presidents son-in-law and senior adviser , will stay now that he has lost his top-secret security clearance; the departure of mr. cohn further shrinks the number of allies mr. kushner and his wife , ivanka trump , have in the white house .  more than one in three top white house officials left by the end of mr. trumps first year and fewer than half of the 12 positions closest to the president are still occupied by the same people as when he came into office , according to a brookings institution study .  mr. cohns departure will bring the turnover number to 43 percent , according to updated figures compiled by kathryn dunn tenpas of the brookings institution .  for all the swings of the west wing revolving door over the last year , mr. cohns decision to leave struck a different chord for people . he is among the most senior officials to resign to date .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_string = \"gary d. cohn , president trumps top economic adviser , said on tuesday that he would resign . more than one in three top white house officials left by the end of mr. trumps first year .\"\n",
    "summ_string = \"angela merkel visited president trump last wednesday . they talked about the iran deal and trade . president trump will travel on to europe next week .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_text = sequence.pad_sequences(tokenizer.texts_to_sequences([text_string]), maxlen=maxlen_text)\n",
    "predict_summ = sequence.pad_sequences(tokenizer.texts_to_sequences([summ_string]), maxlen=maxlen_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46543026]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([predict_text, predict_summ], batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49655700076511095"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(targets_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tok_path, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import scandir, listdir\n",
    "#read in article texts, baseline summs, m1 summs, m2 summs and ref summs\n",
    "\n",
    "article_dir = \"/home/oala/Documents/MT/data/datasets/finished_files/test_output/articles/\"\n",
    "reference_dir = \"/home/oala/Documents/MT/data/datasets/finished_files/test_output/reference/\"\n",
    "baseline_dir = \"/home/oala/Documents/MT/data/datasets/finished_files/test_output/baseline/\"\n",
    "pointergen_dir = \"/home/oala/Documents/MT/data/datasets/finished_files/test_output/pointer-gen/\"\n",
    "pointergencov_dir = \"/home/oala/Documents/MT/data/datasets/finished_files/test_output/pointer-gen-cov/\"\n",
    "\n",
    "article_files = listdir(article_dir)\n",
    "article_files.sort()\n",
    "reference_files = listdir(reference_dir)\n",
    "reference_files.sort()\n",
    "baseline_files = listdir(baseline_dir)\n",
    "baseline_files.sort()\n",
    "pointergen_files = listdir(pointergen_dir)\n",
    "pointergen_files.sort()\n",
    "pointergencov_files = listdir(pointergencov_dir)\n",
    "pointergencov_files.sort()\n",
    "\n",
    "#read in texts\n",
    "texts = []\n",
    "for txt_file in article_files:\n",
    "    with open(article_dir+txt_file,'r',encoding='utf-8', errors='ignore') as txt:\n",
    "        text = txt.read()\n",
    "        text = text.replace('(', '-lrb-')\n",
    "        text = text.replace(')', '-rrb-')\n",
    "        text = text.replace('[', '-lsb-')\n",
    "        text = text.replace(']', '-rsb-')\n",
    "        text = text.replace('{', '-lcb-')\n",
    "        text = text.replace('}', '-rcb-')\n",
    "        texts.append(text)\n",
    "texts = numpy.array(texts)\n",
    "texts_text = numpy.copy(texts)\n",
    "texts = tokenizer.texts_to_sequences(texts)\n",
    "texts = sequence.pad_sequences(texts, maxlen=maxlen_text)\n",
    "\n",
    "#helper functions for summs\n",
    "def summ_dir2array(dir_name, file_list):\n",
    "    summs = []\n",
    "    for txt_file in file_list:\n",
    "        with open(dir_name+txt_file,'r',encoding='utf-8', errors='ignore') as txt:\n",
    "            summ = \"\"\n",
    "            line = txt.readline()\n",
    "            while line:\n",
    "                line = line.replace('\\n', ' ')\n",
    "                line = line.replace('(', '-lrb-')\n",
    "                line = line.replace(')', '-rrb-')\n",
    "                line = line.replace('[', '-lsb-')\n",
    "                line = line.replace(']', '-rsb-')\n",
    "                line = line.replace('{', '-lcb-')\n",
    "                line = line.replace('}', '-rcb-')\n",
    "                \n",
    "                summ += line\n",
    "                line = txt.readline()\n",
    "\n",
    "            summs.append(summ)\n",
    "    summs = numpy.array(summs)\n",
    "    summs_text = numpy.copy(summs)\n",
    "    summs = tokenizer.texts_to_sequences(summs)\n",
    "    summs = sequence.pad_sequences(summs, maxlen=maxlen_summ)\n",
    "    \n",
    "    return summs, summs_text\n",
    "\n",
    "#reference summs\n",
    "reference_summs, reference_summs_text = summ_dir2array(reference_dir, reference_files)\n",
    "\n",
    "#baseline summs\n",
    "baseline_summs, baseline_summs_text = summ_dir2array(baseline_dir, baseline_files)\n",
    "\n",
    "#pointergen summs\n",
    "pointergen_summs, pointergen_summs_text = summ_dir2array(pointergen_dir, pointergen_files)\n",
    "\n",
    "#pointergencov summs\n",
    "pointergencov_summs, pointergencov_summs_text = summ_dir2array(pointergencov_dir, pointergencov_files)\n",
    "\n",
    "N = len(texts_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11490/11490 [==============================] - 7s 618us/step\n",
      "11490/11490 [==============================] - 7s 620us/step\n",
      "[0.3157948085323326, 0.8859878072695072]\n",
      "0.11401218450826806\n"
     ]
    }
   ],
   "source": [
    "#reference\n",
    "reference_preds = model.predict([texts, reference_summs], batch_size=batch_size, verbose=1)\n",
    "print(model.evaluate([texts, reference_summs],[0]*texts.shape[0], batch_size=batch_size))\n",
    "\n",
    "reference_preds_flat = numpy.ndarray.flatten(reference_preds)\n",
    "reference_preds_onehot = numpy.copy(reference_preds_flat)\n",
    "reference_preds_onehot[reference_preds_onehot<0.5]=0\n",
    "reference_preds_onehot[reference_preds_onehot>=0.5]=1\n",
    "#reference_preds_onehot[reference_preds_onehot<0.02]=0\n",
    "#reference_preds_onehot[reference_preds_onehot>=0.02]=1\n",
    "print(sum(reference_preds_onehot)/reference_preds_onehot.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11490/11490 [==============================] - 7s 616us/step\n",
      "0.18398607484769364\n"
     ]
    }
   ],
   "source": [
    "#baseline\n",
    "baseline_preds = model.predict([texts, baseline_summs], batch_size=batch_size, verbose=1)\n",
    "\n",
    "baseline_preds_flat = numpy.ndarray.flatten(baseline_preds)\n",
    "baseline_preds_onehot = numpy.copy(baseline_preds_flat)\n",
    "baseline_preds_onehot[baseline_preds_onehot<0.5]=0\n",
    "baseline_preds_onehot[baseline_preds_onehot>=0.5]=1\n",
    "#baseline_preds_onehot[baseline_preds_onehot<0.02]=0\n",
    "#baseline_preds_onehot[baseline_preds_onehot>=0.02]=1\n",
    "print(sum(baseline_preds_onehot)/baseline_preds_onehot.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11490/11490 [==============================] - 7s 624us/step\n",
      "0.2614447345517842\n"
     ]
    }
   ],
   "source": [
    "#pointergen\n",
    "pointergen_preds = model.predict([texts, pointergen_summs], batch_size=batch_size, verbose=1)\n",
    "\n",
    "pointergen_preds_flat = numpy.ndarray.flatten(pointergen_preds)\n",
    "pointergen_preds_onehot = numpy.copy(pointergen_preds_flat)\n",
    "pointergen_preds_onehot[pointergen_preds_onehot<0.5]=0\n",
    "pointergen_preds_onehot[pointergen_preds_onehot>=0.5]=1\n",
    "#pointergen_preds_onehot[pointergen_preds_onehot<0.02]=0\n",
    "#pointergen_preds_onehot[pointergen_preds_onehot>=0.02]=1\n",
    "print(sum(pointergen_preds_onehot)/pointergen_preds_onehot.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11490/11490 [==============================] - 7s 619us/step\n",
      "0.37885117493472587\n"
     ]
    }
   ],
   "source": [
    "#pointergencov\n",
    "pointergencov_preds = model.predict([texts, pointergencov_summs], batch_size=batch_size, verbose=1)\n",
    "\n",
    "pointergencov_preds_flat = numpy.ndarray.flatten(pointergencov_preds)\n",
    "pointergencov_preds_onehot = numpy.copy(pointergencov_preds_flat)\n",
    "pointergencov_preds_onehot[pointergencov_preds_onehot<0.5]=0\n",
    "pointergencov_preds_onehot[pointergencov_preds_onehot>=0.5]=1\n",
    "#pointergencov_preds_onehot[pointergencov_preds_onehot<0.02]=0\n",
    "#pointergencov_preds_onehot[pointergencov_preds_onehot>=0.02]=1\n",
    "print(sum(pointergencov_preds_onehot)/pointergencov_preds_onehot.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save:\n",
    "    model.save('%s_%s.h5' % (model_class,model_num))\n",
    "    with open('%s_%s_TOKENIZER.pickle' % (model_class,model_num), 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nepal civil war aftermath inspired maggie doyne to help children . doyne 's blinknow foundation supports a home for 50 children and a school that educates hundreds more . do you know a hero ? nominations are open for 2015 cnn heroes .\n",
      "surkhet , nepal -lrb- cnn -rrb- ten years ago , with her high school diploma and a backpack , maggie doyne left her new jersey hometown to travel the world before college . she lived in a buddhist monastery , helped rebuild a sea wall in fiji , then went to india and worked with nepalese refugees . there , she met a young girl who wanted to find her family in nepal . doyne went with her . that 's when doyne 's life took an unexpected turn . do you know a hero ? nominations are open for cnn heroes 2015 . a decade-long civil war had just ended in the country , and doyne witnessed its effects firsthand . she met women and children who were suffering , struggling to survive . `` it changed me , '' said doyne , now 28 . `` there were children with mallets that would go into the riverbed , pick up a big stone and break it into little , little pieces -lrb- to sell -rrb- . and they were doing that all day , every day . '' doyne called her parents and asked them to wire her the $ 5,000 she had earned babysitting . in 2006 , she purchased land in surkhet , a district in western nepal . she worked for two years with the local community to build the kopila valley children 's home . today , kopila -- which means `` flower bud '' in nepali -- is home to about 50 children , from infants to teenagers . doyne started the blinknow foundation to support and grow her efforts . in 2010 , the group opened its kopila valley school , which today educates more than 350 students . doyne lives in nepal year-round , traveling to the u.s. a few times a year . see more cnn heroes . the cnn heroes team traveled to surkhet and talked to doyne about her work and the community she supports . below is an edited version of their conversation . cnn : how does it work , raising nearly 50 kids ? maggie doyne : it 's communal living , for sure ! we 're a family of almost 50 kids ages 8 months to 16 years . everybody just pitches in and helps each other . they all have their chores . they all have their duties . and everybody cooks the meals together and makes sure that they do their part to make the home run smoothly . the staff at the home , we call them the aunties and the uncles . we wake up in the morning and go off to school . and then come home and do homework and eat our meals together , and everybody goes to bed at night . cnn : how does a child come to live in your home ? doyne : our first priority as an organization is to keep a child with their family if at all possible . in order to come into the home , you need to have lost both parents , or in some rare cases have suffered extreme neglect , abuse or have a parent who 's incarcerated . we have to conduct a full investigation . so usually that involves going to the child 's village , making calls , doing police checks , getting documentation and paperwork . we have to dig up birth certificates , death certificates , make sure that everything lines up the way that they say it does . cnn : meanwhile , you have 350 children attending your school . what is their background ? doyne : every single year we 'll get from 1,000 to 1,500 applicants . and we choose the ones who are the most needful and really wo n't be in school without us . most of them live in one room , a mud hut . a lot of them are just in survival mode . we try to relieve the burden from the family , so that the child has food , medical care , books , zero fees for education . cnn : what have you learned working with the local community in nepal ? doyne : i learned very early on , from the beginning , that i could n't come in and just be like , `` here , i have a vision . this is what we 're going to do . '' that does n't work . it has to be slow ; it has to be organic . and it has to come from the community and be a `` we '' thing . it 's really important to me that this is a nepali project , working for nepal , for the community . so the faces that you see are strong nepali women and amazing nepali role-model men . cnn : how does the project continue to grow ? doyne : we started with the home and then school . we run the school lunch program . then we needed to keep our kids really healthy , so we started a small clinic and then a counseling center . from there we started getting more sustainable and growing our own food . and then from there we decided to start a women 's center . we just bought a new piece of property to create a totally green and sustainable off-the-grid campus . this year we converted to solar energy . so we 'll have a high school and then a day care , preschool , elementary , all the way up , and a vocational center where kids can become a thriving young adult with everything they need to succeed moving forward . it 's become so much more than just a little girl with a backpack and a big dream . it 's become a community . and i want to teach and have other people take this example and hope this sets a precedent for what our world can be and look like . want to get involved ? check out the blinknow foundation website at www.blinknow.org and see how to help .\n",
      "-lrb- cnn -rrb- one of the youngest suspects yet has been arrested on terror-related charges in england . a 14-year-old boy was taken into custody after encouraging an attack on an australian parade honoring the war dead and urging the beheading of `` someone in australia , '' deborah walsh , deputy head of counter terrorism at the crown prosecution service , said in a statement thursday . the teenager was taken into custody april 2 after uk 's greater manchester police examined electronic devices and discovered communications between the teen and a man in australia , police said in a statement . the teenager , arrested in blackburn , lancashire , was not named `` because of legal reasons , '' the statement said . he was charged with two counts of inciting another person to commit an act of terrorism overseas and will appear in westminster magistrate 's court on friday . he was communicating with suspects in operation rising , an australian law enforcement operation that apprehended several men suspected of planning terrorist actions , police in victoria , australia , said on the department website . australia : charges in foiled ` isis-inspired ' plot . those acts of terror were planned for anzac day -lrb- australia and new zealand army corps day -rrb- on friday , the centennial of the gallipoli campaign in world war i , police said . `` the first allegation is that , between 15 and 26 march 2015 , the defendant incited another person to commit an act of terrorism , namely to carry out an attack at an anzac parade in australia with the aim of killing and/or causing serious injury to people , '' walsh said . `` the second allegation is that on 18 march 2015 , the defendant incited another person to behead someone in australia . '' australian law enforcement officers arrested several people last weekend in operation rising . tuesday , victoria police and the australian federal police charged sevdet ramdan besim with conspiracy to commit acts done in preparation for , or planning , terrorist acts . authorities have not named the person with whom the 14-year-old in britain was communicating . british teens face terror charges after being detained en route to syria . cnn 's alexander felton contributed to this report .\n",
      "the 14-year-old had communicated with terror suspects in australia , authorities said . police : the teenager encouraged others to attack a parade and behead someone in australia .\n"
     ]
    }
   ],
   "source": [
    "print(reference_summs_text[500])\n",
    "print(texts_text[500])\n",
    "print(clean_2d[500,0])\n",
    "print(clean_2d[500,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on actual testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load model\n",
    "model = load_model('/home/oala/Documents/MT/data/model-params/exciting-crazy/%s/%s/%s_%s.h5' % (model_class,model_num,model_class, model_num))\n",
    "tok_path= '/home/oala/Documents/MT/data/model-params/exciting-crazy/%s/%s/%s_%s_TOKENIZER.pickle' % (model_class,model_num,model_class,model_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100/0 (clean/noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading clean data...\n",
      "...done!\n",
      "Read in candidate noise points...\n",
      "2872\n",
      "(2872,)\n",
      "(11490, 2)\n",
      "(2872, 2)\n",
      "...done!\n",
      "Preprocess clean data, i.e. remove <s> and </s>...\n",
      "...done!\n"
     ]
    }
   ],
   "source": [
    "filename = \"/home/oala/Documents/MT/data/datasets/finished_files/test.bin\"\n",
    "noise_candidates_path = '/home/oala/Documents/MT/noising/4-beam-PGC-noise-on-train/pretrained_model_tf1.2.1/decode_train_400maxenc_4beam_35mindec_120maxdec_ckpt-238410/decoded/'\n",
    "nc_dist = (0.5,0.5)\n",
    "replace = False\n",
    "corr_sample = False\n",
    "separate = False\n",
    "band_width = 4\n",
    "dgp = \"random\"\n",
    "clean_2d, noise_2d = get_data(filename, nc_dist, replace, corr_sample, separate, band_width,noise_candidates_path, dgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11490, 2)\n",
      "(11488, 2)\n"
     ]
    }
   ],
   "source": [
    "print(clean_2d.shape)\n",
    "print(noise_2d.shape)\n",
    "#clean_2d = clean_2d[0:2] #here you reduce clean to 2 datapoints to get only noise!\n",
    "noise_2d = noise_2d[0:2]\n",
    "val_share = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dist:  0.00010876658690450294\n",
      "val dist:  0.0004351610095735422\n"
     ]
    }
   ],
   "source": [
    "texts_train, summs_train, targets_train, texts_val, summs_val, targets_val, tokenizer = \\\n",
    "    prep_data(clean_2d, noise_2d, max_features, val_share, maxlen_text, maxlen_summ,\n",
    "              load_tok=True, tok_path=tok_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the splits to get all data\n",
    "predict_text = numpy.concatenate((texts_train, texts_val))\n",
    "predict_summ = numpy.concatenate((summs_train, summs_val))\n",
    "predict_y = numpy.concatenate((targets_train, targets_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11492/11492 [==============================] - 7s 629us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20658960570462936, 0.9388269976357155]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([predict_text, predict_summ], predict_y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50/50 (clean/noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading clean data...\n",
      "...done!\n",
      "Read in candidate noise points...\n",
      "2872\n",
      "(2872,)\n",
      "(11490, 2)\n",
      "(2872, 2)\n",
      "...done!\n",
      "Preprocess clean data, i.e. remove <s> and </s>...\n",
      "...done!\n"
     ]
    }
   ],
   "source": [
    "filename = \"/home/oala/Documents/MT/data/datasets/finished_files/test.bin\"\n",
    "noise_candidates_path = '/home/oala/Documents/MT/noising/4-beam-PGC-noise-on-train/pretrained_model_tf1.2.1/decode_train_400maxenc_4beam_35mindec_120maxdec_ckpt-238410/decoded/'\n",
    "nc_dist = (0.5,0.5)\n",
    "replace = False\n",
    "corr_sample = False\n",
    "separate = False\n",
    "band_width = 4\n",
    "dgp = \"random\"\n",
    "clean_2d, noise_2d = get_data(filename, nc_dist, replace, corr_sample, separate, band_width,noise_candidates_path, dgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11490, 2)\n",
      "(11488, 2)\n"
     ]
    }
   ],
   "source": [
    "print(clean_2d.shape)\n",
    "print(noise_2d.shape)\n",
    "#noise_2d = noise_2d[0:2] #here you reduce clean to 2 datapoints to get only noise!\n",
    "val_share = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dist:  0.4993200239351575\n",
      "val dist:  0.5025027203482045\n"
     ]
    }
   ],
   "source": [
    "texts_train, summs_train, targets_train, texts_val, summs_val, targets_val, tokenizer = \\\n",
    "    prep_data(clean_2d, noise_2d, max_features, val_share, maxlen_text, maxlen_summ,\n",
    "              load_tok=True, tok_path=tok_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the splits to get all data\n",
    "predict_text = numpy.concatenate((texts_train, texts_val))\n",
    "predict_summ = numpy.concatenate((summs_train, summs_val))\n",
    "predict_y = numpy.concatenate((targets_train, targets_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22978/22978 [==============================] - 14s 623us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3952894529908317, 0.8331882612223164]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([predict_text, predict_summ], predict_y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0/100 (clean/noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading clean data...\n",
      "...done!\n",
      "Read in candidate noise points...\n",
      "2872\n",
      "(2872,)\n",
      "(11490, 2)\n",
      "(2872, 2)\n",
      "...done!\n",
      "Preprocess clean data, i.e. remove <s> and </s>...\n",
      "...done!\n"
     ]
    }
   ],
   "source": [
    "filename = \"/home/oala/Documents/MT/data/datasets/finished_files/test.bin\"\n",
    "noise_candidates_path = '/home/oala/Documents/MT/noising/4-beam-PGC-noise-on-train/pretrained_model_tf1.2.1/decode_train_400maxenc_4beam_35mindec_120maxdec_ckpt-238410/decoded/'\n",
    "nc_dist = (0.5,0.5)\n",
    "replace = False\n",
    "corr_sample = False\n",
    "separate = False\n",
    "band_width = 4\n",
    "dgp = \"random\"\n",
    "clean_2d, noise_2d = get_data(filename, nc_dist, replace, corr_sample, separate, band_width,noise_candidates_path, dgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11490, 2)\n",
      "(11488, 2)\n"
     ]
    }
   ],
   "source": [
    "print(clean_2d.shape)\n",
    "print(noise_2d.shape)\n",
    "clean_2d = clean_2d[0:2] #here you reduce clean to 2 datapoints to get only noise!\n",
    "val_share = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dist:  0.9997824194952132\n",
      "val dist:  1.0\n"
     ]
    }
   ],
   "source": [
    "texts_train, summs_train, targets_train, texts_val, summs_val, targets_val, tokenizer = \\\n",
    "    prep_data(clean_2d, noise_2d, max_features, val_share, maxlen_text, maxlen_summ,\n",
    "              load_tok=True, tok_path=tok_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the splits to get all data\n",
    "predict_text = numpy.concatenate((texts_train, texts_val))\n",
    "predict_summ = numpy.concatenate((summs_train, summs_val))\n",
    "predict_y = numpy.concatenate((targets_train, targets_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11490/11490 [==============================] - 7s 622us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.587638956993118, 0.7233246312751471]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([predict_text, predict_summ], predict_y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_clean_preds = numpy.ndarray((reference_preds.shape[0],1), dtype='int')\n",
    "pure_clean_preds[reference_preds<0.5] = 0\n",
    "pure_clean_preds[reference_preds>=0.5] = 1\n",
    "pure_clean_preds_one_hot = numpy.zeros((pure_clean_preds.shape[0],2))\n",
    "pure_clean_preds_one_hot[numpy.arange(reference_preds.shape[0]),pure_clean_preds[:,0]]=1\n",
    "pure_clean_targets_one_hot = numpy.ones((pure_clean_preds.shape[0],2))\n",
    "pure_clean_targets_one_hot[:,1]=0\n",
    "\n",
    "one = model.predict([predict_text, predict_summ], batch_size=batch_size)\n",
    "pure_noise_preds = numpy.ndarray((one.shape[0],1), dtype='int')\n",
    "pure_noise_preds[one<0.5] = 0\n",
    "pure_noise_preds[one>=0.5] = 1\n",
    "pure_noise_preds_one_hot = numpy.zeros((pure_noise_preds.shape[0],2))\n",
    "pure_noise_preds_one_hot[numpy.arange(one.shape[0]),pure_noise_preds[:,0]]=1\n",
    "pure_noise_targets_one_hot = numpy.ones((pure_noise_preds.shape[0],2))\n",
    "pure_noise_targets_one_hot[:,0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9898.,    0.],\n",
       "       [1592.,    0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_clean_preds_one_hot.T @ pure_clean_targets_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0.,  1302.],\n",
       "       [    0., 10188.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_noise_preds_one_hot.T @ pure_noise_targets_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.999446  ],\n",
       "       [0.33823976],\n",
       "       [0.8246137 ],\n",
       "       ...,\n",
       "       [0.98373276],\n",
       "       [0.99958295],\n",
       "       [0.99338466]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([predict_text, predict_summ], batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
