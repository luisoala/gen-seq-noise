{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cell-width control\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#packages\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "#utils\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import struct\n",
    "import time\n",
    "from noise import *\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Concatenate, Dot, Embedding, LSTM, Conv1D, MaxPooling1D, Input, Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/media/oala/4TB/experimental-data/output-scoring/baseline/'\n",
    "summ_load_path = '/home/oala/Documents/MT/data/datasets/finished_files/test_output/baseline/'\n",
    "text_load_path = '/home/oala/Documents/MT/data/datasets/finished_files/test_output/articles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_decoded_fpaths) 11490\n",
      "len(all_article_fpaths) 11490\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(47)\n",
    "\n",
    "partition = {}\n",
    "partition['test'] = []\n",
    "labels = {}\n",
    "id_counter = 1\n",
    "\n",
    "#NOISE\n",
    "#get lists of filenames from text dir and summ dir\n",
    "generator_sample_size = 1\n",
    "all_article_fpaths = []\n",
    "all_decoded_fpaths = []\n",
    "    \n",
    "#first deal with decoded summs\n",
    "file_path_decoded= summ_load_path\n",
    "decoded_fnames = sorted(os.listdir(file_path_decoded))\n",
    "decoded_fpaths = []\n",
    "\n",
    "filenum_old = 'XXXXXX'\n",
    "name_old = 'XXXXXX'\n",
    "alarm_log = {}\n",
    "alarm_count = 0\n",
    "count = 10\n",
    "\n",
    "#find bad indices\n",
    "# for i in range(len(decoded_fnames)):\n",
    "#     filenum_new = decoded_fnames[i][0:6]\n",
    "#     if filenum_new == filenum_old:\n",
    "#         count += 1\n",
    "#     if filenum_new != filenum_old:\n",
    "#         if count != 10:\n",
    "#             alarm_count += 1\n",
    "#             alarm_log[name_old] = count #collect all the bad keys with count\n",
    "#         count = 1\n",
    "#     filenum_old = filenum_new\n",
    "#     name_old = decoded_fnames[i]\n",
    "# all_bad_indices = [int(filename[0:6]) for filename in alarm_log.keys()] #now have all the bad indices\n",
    "all_bad_indices = []\n",
    "\n",
    "#only add good indices\n",
    "for i in range(len(decoded_fnames)):\n",
    "    if int(decoded_fnames[i][0:6]) not in all_bad_indices:\n",
    "        decoded_fpaths.append(file_path_decoded+decoded_fnames[i])\n",
    "\n",
    "all_decoded_fpaths = all_decoded_fpaths + decoded_fpaths\n",
    "\n",
    "#then deal with articles\n",
    "file_path_articles = text_load_path\n",
    "article_fnames = sorted(os.listdir(file_path_articles))\n",
    "article_fpaths = []\n",
    "for i in range(len(article_fnames)):\n",
    "    article_fpaths.append(file_path_articles+article_fnames[i])\n",
    "article_fpaths = numpy.array(article_fpaths, dtype='object')\n",
    "\n",
    "article_fpaths = numpy.delete(article_fpaths,all_bad_indices)\n",
    "article_fpaths = list(numpy.repeat(article_fpaths,generator_sample_size))\n",
    "all_article_fpaths = all_article_fpaths + article_fpaths\n",
    "\n",
    "#sample N_noise indices from len(lists filenames) wout replacement\n",
    "print('len(all_decoded_fpaths)',len(all_decoded_fpaths))\n",
    "print('len(all_article_fpaths)',len(all_article_fpaths))\n",
    "\n",
    "noise_indices = numpy.random.choice(len(all_decoded_fpaths), len(all_article_fpaths)//generator_sample_size, replace=False)\n",
    "for i in noise_indices:\n",
    "    #read text_files[i] and generator_summ[i]\n",
    "    #preprocess string per your needs\n",
    "    with open(all_article_fpaths[i], 'r') as article_file:\n",
    "        text = article_file.read()\n",
    "        text = text.replace('(', '-lrb-')\n",
    "        text = text.replace(')', '-rrb-')\n",
    "        text = text.replace('[', '-lsb-')\n",
    "        text = text.replace(']', '-rsb-')\n",
    "        text = text.replace('{', '-lcb-')\n",
    "        text = text.replace('}', '-rcb-')\n",
    "    with open(all_decoded_fpaths[i], 'r') as decoded_file:\n",
    "        summ = decoded_file.read()\n",
    "        summ  = summ.replace('\\n', ' ')\n",
    "        summ  = summ.replace('(', '-lrb-')\n",
    "        summ  = summ.replace(')', '-rrb-')\n",
    "        summ  = summ.replace('[', '-lsb-')\n",
    "        summ  = summ.replace(']', '-rsb-')\n",
    "        summ  = summ.replace('{', '-lcb-')\n",
    "        summ  = summ.replace('}', '-rcb-')\n",
    "    #save string as before\n",
    "    #update labels and counter\n",
    "    store_string = text+'\\n'+summ\n",
    "    id_name = 'id-'+str(id_counter)\n",
    "    with open(save_path+id_name, 'w') as file:\n",
    "        file.write(store_string)\n",
    "    partition['test'] += [id_name]\n",
    "    labels[id_name] = 0\n",
    "    id_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11490\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(len(partition['test']))\n",
    "count = 0\n",
    "for key in partition['test']:\n",
    "    count += labels[key]\n",
    "print(count/len(partition['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store label dict...\n",
      "...done!\n",
      "partition dict...\n",
      "...done!\n"
     ]
    }
   ],
   "source": [
    "#store label dict\n",
    "print('store label dict...')\n",
    "with open(save_path+'labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('...done!')\n",
    "\n",
    "#store partition dict\n",
    "print('partition dict...')\n",
    "with open(save_path+'partition.pickle', 'wb') as handle:\n",
    "    pickle.dump(partition, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('...done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
